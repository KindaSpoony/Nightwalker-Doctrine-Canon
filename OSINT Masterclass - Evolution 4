Digital Battlefield Deception and Counter-OSINT Tactics (Masterclass Evolution 4)

Situation

Modern conflict zones have become a digital battlefield where information itself is contested. Adversaries employ denial, spoofing, and information camouflage tactics to mislead observers and deceive OSINT analysts ￼. In Ukraine (2022) and Gaza (2023), we witnessed both the power of OSINT and the ease with which false narratives can spread. For example, Russian proxies hacked a Ukrainian news feed to broadcast a deepfake of President Zelenskyy surrendering, amplifying it on Telegram – a deception operation so bold that it briefly tricked some troops ￼. Likewise, during the 2023 Gaza conflict, misdated videos and imposter accounts muddied the truth: an official social media post by an ally was quickly retracted after OSINT journalists noticed its video timestamp was wrong ￼, and old conflict footage was repackaged as “new” to inflame emotions ￼. These incidents underscore that while open-source intelligence is invaluable, it is also vulnerable to manipulation – adversaries can deliberately plant incorrect information in public sources ￼. NATO doctrine has accordingly highlighted the need to both camouflage friendly operations and detect enemy deception in the digital domain ￼. In short, the operating environment is one where “a stray Instagram photo or crowdsourcing campaign” can have outsized effects ￼, and analysts must be prepared to navigate a fog of war compounded by tweets, deepfakes, and propaganda.

Mission

Our mission is to train OSINT analysts to detect, counter, and ethically conduct digital deception operations in modern conflict environments. Analysts will learn to recognize adversary OSINT denial, spoofing, and information camouflage tactics, and to respond with appropriate countermeasures. We will emphasize ethical decision-making throughout – ensuring analysts can uphold truth and intelligence integrity when facing disinformation, conflicting coalition narratives, or pressure to deceive. By the end of this evolution, trainees will be equipped to expose adversary lies, protect friendly information, and employ deception ethically in support of operational goals. End state: a cadre of OSINT professionals capable of operating in a contested information environment, denying the enemy’s influence while safeguarding our coalition’s narrative and values.

Execution

Concept of Operations: This evolution is structured as a five-part practical exercise following the operations order format. Trainees will proceed through scenario-based modules that incorporate real-world social media artifacts, red-team adversary profiles, coalition friction simulations, and visual analytical aids. They will alternate between detecting deception (as analysts) and employing deception (as ethical red-teamers) under guided mentorship. Key doctrinal principles from JP 2-0, JP 3-13, MCDP-2, the NATO OSINT Handbook, and ODNI best practices are integrated into each phase to align training with official guidance.

Real-World Social Media Artifacts: Trainees will analyze a curated set of historical social media posts and OSINT images from conflicts like Ukraine 2022 and Gaza 2023. Each artifact is timestamped, archived, anonymized, and tagged per VAULTIS standards for authenticity. For example, one Twitter post (March 2022) depicts a supposed Ukrainian surrender announcement – a fake post derived from a deepfake video that was debunked within hours ￼. Another artifact is a Telegram channel screenshot from 2023 showing inflated casualty figures accompanied by doctored images. Analysts must verify each artifact, identify indicators of falsification (e.g. inconsistent timestamps, metadata or known propaganda narratives), and differentiate between “spoofed” vs. real OSINT. All artifacts are VAULTIS-tagged (Visible, Accessible, Understandable, Linked, Trustworthy, Interoperable, Secure) to ensure transparency and trust ￼ – for instance, metadata showing original source and time is provided, but personal identifiers are removed to protect privacy. By examining these real-world examples, students practice assessing information credibility under time pressure, learning to spot recycled footage, imposter accounts, deepfake telltales, and other deception signatures in the wild.

Adversary Red Team Profiles: We have developed four adversary profiles to emulate red-team deception tactics commonly used by state and non-state actors. Each profile includes the adversary’s techniques (TTPs), digital “signature,” and preferred spoofing vectors, so trainees can learn their playbook:
	•	Russia (Maskirovka & Telegram Distortion): Emulates Russia’s tradition of maskirovka – a doctrine encompassing everything from camouflage and decoys to strategic disinformation ￼. The red team will simulate tactics like flooding Telegram channels with false narratives, deploying fake personas (“bots/trolls”) and forged documents, and using denial-and-deception measures to confuse our collection. Trainees must recognize these measures (e.g. sudden chatter about a fake offensive, or look-alike news sites) and practice counter-analysis. This profile highlights how Russia blends classic deception (decoys, feints) with modern info-warfare (hacked media broadcasts, fake fact-checking services ￼).
	•	Iran (Controlled Leaks & Silence Campaigns): Emulates Iranian cyber operations and propaganda approaches. The red team might release a “controlled leak” – for instance, a leaked document or video ostensibly exposing enemy secrets – at a timed moment to distract from Iran’s real activities. (One real example: an apparent dissident’s claim of an official’s assassination that turned out false, suspected to be an Iranian info op to distract from actual protest crackdowns ￼.) They also simulate “silence campaigns” by creating information blackouts – e.g. mass reporting or cyber attacks to shut down social media during key moments – reflecting Iran’s habit of internet shutdowns and strict censorship to control narratives. Iranian digital signatures include clumsy but sweeping tactics to “distract, discredit, distort, and sow distrust” in adversaries’ discourse ￼. Trainees will learn to verify leaks (to avoid enemy disinfo traps) and respond to or work around information blackouts.
	•	ISIS (Chaotic Signal Flooding & Coded Messaging): Emulates the information tactics of ISIS and similar violent extremist organizations. This profile will flood the information space with chaotic signals – e.g. a blitz of propaganda videos, audio messages, and claims on multiple platforms – to overwhelm defenders’ analysis capacity. (ISIS has famously used Telegram as its “app of choice” to spread propaganda and coordinate recruits ￼, often re-posting content faster than it can be taken down.) The red team will also employ coded messaging: for instance, hiding instructions in seemingly innocuous images or using phrases from religious texts as operation triggers known only to insiders. Trainees must practice filtering signal from noise – distinguishing genuine threats or intel cues amid a deluge of content ￼. They will also use tools to detect patterns or steganography in ISIS communications. The focus here is on building resilience against information overload and tracing extremist comms without being misled by their deceptive obfuscation.
	•	China (Open Data Harvesting & “Accidental” Disclosures): Emulates Chinese state and state-linked operations in the information domain. China’s profile highlights mass OSINT gathering and selective revelation. The red team will demonstrate how vast open-source data can be weaponized: e.g. scraping personal data on coalition officers from social media (mirroring real Chinese big-data collection efforts that amassed millions of profiles ￼). They will also stage an “accidental” disclosure: for instance, posting (and quickly deleting) a sensitive document or satellite image on a public forum, to gauge reaction or subtly threaten disclosure. Such moves echo cases where Chinese sources leaked military photos or sensitive data, then claimed it was unintentional. Trainees must discern whether a leak is deliberate misinformation or a genuine slip, and respond in line with counterintelligence guidance. This profile also trains analysts on digital signature shaping – recognizing when an actor is masking their hand (e.g. using front companies or fake researcher personas for data gathering, as seen in the Zhenhua data leak case) and how to attribute activity despite the camouflage.

Simulated Coalition Friction Scenarios: Throughout the exercise, trainees will face ethical and procedural dilemmas that test their decision-making when deception operations intersect with coalition information-sharing and public affairs. We inject three main scenarios, each with branching decision paths and doctrinal cues for debrief:
	1.	NATO Release Authority Conflict: In this scenario, an intelligence report derived from OSINT reveals a critical enemy move. However, the report is originally classified REL TO USA only, not releasable to coalition partners due to source sensitivity. As simultaneous open-source evidence emerges on social media, coalition allies request confirmation. Trainees must decide: do they share intel with NATO partners immediately (violating the REL TO restriction), hold it back until formal release is approved, or attempt a “tearline” – a sanitized summary omitting sensitive details? Each choice has consequences: sharing without authorization risks sources; withholding could erode trust or miss an opportunity to counter the enemy; releasing a partial truth might cause confusion. Branching Outcome: If they share improperly, the scenario triggers a rebuke on proper foreign disclosure procedures (citing doctrine on classified info release). If they wait, allied operations may falter. If they use a tearline, they must craft it to be truthful yet secure. Debrief: Instructors highlight doctrine (e.g. foreign disclosure rules in JP 2-0) and discuss how to balance security with coalition intelligence needs, emphasizing that clear communication and prior liaison are key. Ethical reflection focuses on the principle of integrity versus loyalty: upholding security agreements while striving not to mislead partners.
	2.	Conflicting Public Narratives (Allied PAO vs. OSINT): This scenario places trainees in a joint task force intelligence cell following a controversial airstrike. The allied Public Affairs Office (PAO) releases a statement minimizing civilian harm, but OSINT analysts (and social media activists) have evidence suggesting higher casualties and possible mistakes. Trainees, privy to classified BDA (battle damage assessment), notice the official narrative diverges from both OSINT and what they suspect is the truth. They must choose a course: quietly alert command about the discrepancy, attempt to correct the public record via an official OSINT channel, or stay silent to maintain a unified front. Branching Outcome: If they push to correct the narrative publicly, they encounter resistance from PAO and political leadership concerned about public backlash; this might lead to a simulated media leak testing their resolve. If they stay silent, later revelation of the truth causes a loss of credibility for the coalition. If they escalate internally, a compromise press release may be issued. Debrief: Using NATO and ODNI ethical guidelines, instructors review the importance of truth and accuracy in intel and public communication. They reference intelligence ethics (e.g. the ODNI’s principle of “truth” in analysis) and discuss how analysts can ethically navigate loyalty to allies vs. honesty to the public. Trainees reflect on the long-term cost of misinformation: short-term gains in narrative cohesion can backfire when OSINT reveals facts anyway. This inject underlines the need for coordination between intelligence and PAO, and for pre-established crisis communication plans that consider OSINT findings.
	3.	Tearline Dilemma & Doctrinal Redaction: In this final scenario, trainees prepare an OSINT-based “tearline” report for release to coalition partners and possibly to the public. The situation: a high-value target was neutralized using methods that can’t be disclosed; however, plenty of open-source evidence (locals posting photos, etc.) is already circulating. Trainees draft a brief excluding the classified specifics (e.g. drone capabilities, or a partner nation’s involvement). During review, they face an ethical dilemma: the true context (omitted) significantly alters the interpretation of events. For example, the omission of a partner’s role might lead the public to wrongly blame solely U.S. forces for collateral damage that was actually caused by the partner. Trainees must decide how to word the public-facing version – whether to allow a potentially misleading omission or to find a way to hint at the truth without breaking classification. Branching Outcome: Different drafts will have different impacts. A fully candid explanation is disallowed by higher headquarters, triggering a lesson on classification authority. An over-sanitized report passes muster but results in ally public outrage due to misunderstanding. An artfully crafted middle-ground version maintains operational security while mitigating false impressions, and is approved. Debrief: Instructors reference doctrine from JP 3-13 (Information Operations) about honesty and credibility in military information support, and NATO OSINT handbook guidance on sanitization. They walk through an ethical flowchart decision process, examining at each step how to uphold truth, minimize harm, and obey security laws. Trainees reflect on how “duty to truth” and “duty to protect sources” can conflict, and discuss strategies to address such conflicts (e.g. seeking higher-level decision or inter-government consultation). The key takeaway is that deception by omission is still deception – analysts must strive to minimize the negative effects of necessary secrecy on coalition trust and public confidence.

Visual Training Aids: To support these modules, we incorporate visual artifacts that illustrate both deception patterns and countermeasures. For example, a deception signature map is provided for the Russia and China profiles, showing a color-coded overlay of spoofed vs. real OSINT indicators across the area of operations. This map uses red markers for confirmed falsehoods and green for validated truths, helping analysts visualize how an adversary blends fake narratives among real events to confuse observers.

Figure 1: Sample Deception Signature Map – a training artifact highlighting how fabricated OSINT signals (red icons) are interspersed with genuine intelligence reports (green icons) in a conflict scenario. Such overlays enable analysts to practice filtering “noise” from “signal” by seeing the ground truth and deception inputs side-by-side.

We also present a Counter-OSINT Detection Matrix, which is a reference chart mapping common adversary deception tactics to recommended detection and verification techniques. For instance, the matrix might list “Deepfake video spread via Telegram” in one row and the corresponding counter-techniques (such as deepfake video frame analysis, reverse image searches, official source cross-checks) in the adjacent column. Trainees can refer to this matrix during exercises to select the right tool or approach for each deception encountered. Additionally, an Ethical Decision Flowchart is provided (annotated with doctrinal citations) to guide analysts through tough calls. This simple flowchart poses questions like “Is the information verified by at least two reliable sources?” – if no, then DO NOT disseminate (citing JP 2-0’s guidance on analysis and verification); “Would releasing this info violate classification or an ally’s trust?” – if yes, then elevate decision to command. Such visual aids reinforce the lesson by giving clear, doctrinally-grounded reference points for action. Trainees are encouraged to use these aids actively in each scenario and later in real-world planning.

Admin/Logistics

Training Materials: All required artifacts and references are prepared and available via the VAULTIS-compliant training repository. (VAULTIS stands for Visible, Accessible, Understandable, Linked, Trustworthy, Interoperable, Secure ￼ – our materials adhere to these principles to facilitate effective learning.) Each social media artifact is provided in printouts and digital form with archive links and verification hashes to confirm timestamps. Personal data in examples (usernames, faces) have been anonymized or blurred in accordance with privacy and ethical guidelines. Visual aids (maps, matrices, flowcharts) are printed in color and handed out, with larger versions displayed on the projector during discussions. Doctrine excerpts (JP 2-0, JP 3-13, MCDP-2, NATO OSINT Handbook, ODNI best practices) are compiled in a reference reader booklet for each student, with key paragraphs highlighted for quick access during exercises.

Logistics and Support: The training will be conducted in the ISR Operations classroom and the adjacent computer lab. Each analyst will have access to an unclassified network workstation loaded with simulation software and links to OSINT tools (for example, a fake news feed portal for the scenarios, and sandbox accounts on Telegram/X for interactive play). A dedicated “Red Team cell” (instructors role-playing adversaries) will operate in a separate control room to inject live messages and updates into the scenario (e.g. posting in the simulated Telegram channel or responding as coalition HQ when trainees communicate requests). We have scheduled the evolution to run over two days. Day 1 will cover the introductory brief, Situation and Mission overview, and then run Scenario 1 and 2 (with debriefs). Day 2 will cover Scenario 3 and a capstone exercise tying all lessons together, followed by a comprehensive AAR (After-Action Review). Meals, breaks, and technical support are arranged – see the timetable in Annex A for details. All participants are required to sign an ethics and confidentiality agreement given the sensitive nature of deception training; while the content is unclassified, the training environment must remain controlled to preserve the realism of scenarios.

Evaluation: During the scenarios, instructors will observe and mentor in real time, using a checklist to note each analyst’s critical actions (e.g. verifying a source, raising an ethical concern, following proper release procedures). These observations, combined with each trainee’s input in group discussions, will form the basis of an individual feedback report. There is no graded “exam,” but the final capstone requires the team to produce a mock intelligence brief that will be reviewed for accuracy, thoroughness, and adherence to ethical standards. All feedback will be delivered in the final session, along with a doctrinal debrief connecting the lessons learned back to official guidance (for example, reinforcing how trainees applied JP 2-0’s mandate to cross-verify information and avoid adversary deception ￼).

Command/Signal

Command Relationship: This masterclass is conducted under the authority of the Joint Intelligence Training Center (JITC) and overseen by the Coalition J2X (Counterintelligence and HUMINT staff) in coordination with the Information Operations (J3 IO) officer. The Training Director (callsign “Mentor-6”) will serve as the overall instructor-in-charge, acting also as the notional “Higher Headquarters” in the scenarios to whom trainees can reach back. Each adversary red team profile is led by an instructor role-playing an OPFOR commander (e.g. “Red 1” for Russia, “Red 2” for Iran, etc.), reporting to the Training Director for synchronization.

Roles and Responsibilities: Trainees are organized into an OSINT analytical cell (blue team) with a designated team lead for each scenario (the role can rotate to give multiple students leadership experience). The team lead is responsible for consolidating the team’s assessment and making any formal requests or reports to the instructors (who simulate higher command or coalition partners as needed). A white cell (exercise control) will monitor progress and can pause the scenario if needed for safety or teaching points.

Communications Plan: We will use both in-person communication and electronic messaging to simulate real intel workflows. For scenario injects, the red team will use a dedicated Slack workspace (simulating coalition channels) and Telegram groups (for adversary chatter) that all trainees have access to on their workstations. Important “signal” injects (e.g. a critical fake intel drop) will also be announced verbally or via a popup message to ensure it isn’t missed in the noise. Trainees are expected to communicate internally using the Slack channel “BlueTeam-Analysis” for collaboration, and to draft any reports or questions to higher headquarters via the “REPORTS” channel. Outside of simulation play, instructors and students will communicate through normal means (email and face-to-face briefings). If a trainee is uncertain how to proceed or believes an inject violates realism or ethics, they have a safety word (“Time-Out”) they can use to pause and confer with the white cell privately. This ensures training remains ethical and aligned with VAULTIS principles, even as we explore deception—if something feels questionable, we address it immediately in the learning context.

Signal (Reports/Deliverables): The expected outputs from the trainees include: an initial information environment assessment (after reviewing Situation artifacts, they will brief the “Commander” on the baseline OSINT and deception landscape), three spot reports/intel updates during each scenario (responding to key events/injects, following the format given in the reference SOP), and the final capstone OSINT brief. These will be communicated in writing via the simulation channels, and key findings will also be briefed orally in plenary debriefs. The Training Director (Mentor-6) will simulate the role of commanding officer or coalition J2 to whom these products are delivered. All draft products should be clearly marked as Exercise documents to prevent any confusion with real intel. Trainees must practice using proper classification markings in the drafts (e.g. if they include any notional classified info, label it accordingly), even though scenario content is unclassified – this is to ingrain good habits.

Finally, the chain of command for any ethical concerns or scenario issues is emphasized: if any trainee feels a scenario inject or instruction contradicts our professional ethics or doctrinal limits, they are empowered to raise it to the Training Director (or even directly to the JITC Commander if needed). Open discussion is encouraged in the AAR about these aspects, reaffirming our commitment that deception tactics used in training or operations must always remain within the bounds of law and ethics. As a Marine Corps doctrinal publication wisely notes, “We may be the victims of deception… whether by a deliberate enemy effort or by our own preconceptions” ￼ – through this masterclass, command ensures our analysts and systems are prepared to minimize that risk, upholding truth and operational security hand in hand.